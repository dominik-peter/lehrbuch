Theoretische Grundlagen
Aufgrund der Weiterentwicklung der Computernetzwerk Technologien werden immer mehr Angriffe auf Computernetze moeglich und durchgefuehrt. Diese Angriffe erfolgen hauptsaechlich durch die Ausnutzung unbekannter Schwachstellen (oder Bugs), welche in allen Systemen und Applikationen gefunden werden koennen. (Rui, Yongquan, & Mingjun, 2009)
Schwachstelle
Angreifer koennen durch das Ausnutzen einer Schwachstelle Zugriff auf Daten erhalten, auf die sie eigentlich nicht zugreifen duerfen. Je nach Art der Sicherheitsluecke koennen sie die Daten auch manipulieren. Ausserdem erlauben Schwachstellen haeufig die Interaktion mit weiterfuehrenden Systemen, wodurch Aktivitaeten ausgefuehrt werden, auf die der User oder Angreifer normalerweise keinen Zugriff hat. Falls die Applikation mit Administratorrechten ausgefuehrt wird, hat ein Angreifer vollen Zugriff auf die kompromittierte Maschine (Tan et al., 2014).
Eine Schwachstelle wird als Zero-Day Schwachstelle bezeichnet, wenn sie seit dem ersten Release (Tag Null) existiert und nicht vom Hersteller behoben wurde. Diese Art von Schwachstelle ist die gravierendste, da sie auf jeder Softwareversion ausgenutzt werden kann.

Intrusion Detection mit Hilfe von Big Data Analyseverfahren

Intrusion Detection System
Ein Intrusion Detection System (IDS) ist ein Prozess oder ein Geraet, welches Netzwerkereignisse ueberwacht und analysiert, um Aktivitaeten zu erkennen die gegen Sicherheitsrichtlinien verstossen. Es kann aus Hardware, Software, oder aus einer Kombination von beiden bestehen, die das Netzwerk gegen unautorisierte Zugriffe ueberwacht. (Gadal & Mokhtar, 2017)
Das Ziel eines IDS ist es, eine zusaetzliche Sicherheitsschicht gegen boesartige Rechensysteme aufzubauen, indem es Angriffe entdeckt und User warnt, (Tan et al., 2014) sowie Angreifer identifiziert, bevor sie ernsthaften Schaden anrichten koennen. (Gadal & Mokhtar, 2017). Vor allem die adaptive und schnelle Erfassung von unbekannten Angriffsmustern ist ein wichtiger Aspekt der Intrusion Detection, da taeglich neue Angriffsformen entwickelt werden (Kulariya, Saraf, Ranjan, & Gupta, 2016).
Funktionsweise
Intrusion Detection Systeme koennen nach ihren zugrundeliegenden Datenquellen, die fuer die Auswertung verwendet werden, kategorisiert werden. Host-Based IDS (HIDS; Host Basierte IDS) entdecken boesartige Vorfaelle auf Hostmaschinen. Sie befassen sich hauptsaechlich mit Insider Angriffen, die meistens das Ziel haben, an hoehere Berechtigungen (root-Berechtigungen) zu gelangen. Network-Based IDS (NIDS; Netzwerkbasierte IDS) ueberwachen und kennzeichnen gefaehrliche Inhalte, die durch das Netz fliessen, oder geben Hinweise auf bedrohliche Muster. NIDS koennen beispielsweise Port-Scans oder indirekte und direkte Denial of Service (Flooding Attacks) erkennen. Data Leak Prevention Systems (DLPS; Datenleck Vorbeugungssysteme) koennen auch als eine Art von IDS gesehen werden, obwohl deren Fokus eher auf Datensicherheit gelegt ist.
Ein Zusammenspiel zwischen Firewall, IDS und DLPS ist sehr wichtig, da sie sich gegenseitig sehr gut ergaenzen und in Zusammenarbeit auch komplexe Angriffsversuche, sowie Flooding und Insider Angriffe erkennen koennen (Tan et al., 2014)
Konventionelle IDS sind haeufig eigenstehende Systeme, die in Computernetzwerken auf einem Server angesiedelt sind. Sie koennen als Misuse-Based Systems (missbrauchsbasierte Systeme) oder Anomaly-Detection Systems (anomaliebasierte Systeme) eingestuft werden.(Tan et al., 2014)
Misuse-Based Systems
Misuse-Based IDS haben eine sehr hohe Erkennungsgenauigkeit, sind aber anfaellig fuer Zero-Day Angriffe. Der ihnen zugrundeliegende Erkennungsalgorithmus ueberprueft lediglich auf bekannte Angriffssignaturen. Das System ist nicht in der Lage, selbst neue Signaturen fuer einen unbekannten Angriff zu generieren (Tan et al., 2014). Die beliebtesten IDS fallen dank der niedrigen Quote der Falschmeldungen in die Gruppe der Misuse-Based IDS (Rui et al., 2009).
Anomaly-Detection Systems
Anomaly-Detection Systems haben ein hohes Potential, Zero-Day Attacken zu identifizieren. Sie sind allerdings anfaellig dafuer, viele False Positives auszuloesen (Tan et al., 2014). Die Anomalieerkennung basiert auf einer Modellierung der normalen Aktivitaeten eines Systems. Es ist nicht einfach, das normale Verhaltensprofil eines Systems oder Benutzers zu beobachten, da der beobachtete Zeitraum Spuren eines Angriffs enthalten kann. Ausserdem verhalten sich User oft anders als ihr „Normalprofil“ es vermuten laesst, wodurch ebenfalls ein falscher Alarm ausgeloest werden koennte (Rui et al., 2009).
Big Data Analyseverfahren
Die meisten der IDS, die in den letzten drei Jahrzehnten entwickelt wurden, sind Misuse-Based IDS. Da sowohl dauerhaft neue Angriffe entdeckt und verwendet werden, als auch die Computernetze stetig weiterentwickelt (und damit schneller) werden, sollten IDS so wenig Rechenleistung wie moeglich verbrauchen, sowie hohe Erkennungsraten haben (Rui et al., 2009).
Intrusion Detection mit Hilfe von Big Data Analyseverfahren

Ein IDS muss heutzutage ein vielfaches mehr Daten verarbeiten als bisher, sowie zwischen einer hoeheren Zahl an Angriffen unterscheiden koennen. Konventionelle Werkzeuge und Techniken sind nicht mehr angemessen, da sie eine vergleichsweise hohe Durchlaufzeit haben und nicht mit der Menge an Daten mithalten koennen (Kulariya et al., 2016).
Dieses Kapitel widmet sich Big Data Ansaetzen. Sie wurden dafuer entwickelt, grosse Datenmengen schnell zu verarbeiten und versprechen deshalb bessere Ergebnisse, obwohl sie zur Berechnung weniger Zeit in Anspruch nehmen.
Maschinelles Lernen
Das Maschinelle Lernen ist eines der Kernelemente bei der Verarbeitung von Big Data und bezeichnet die Generierung von Wissen aus einem Beispieldatensatz. Es wird nach wiederkehrenden Mustern und kausalen Zusammenhaengen gesucht, um bei unbekannten Daten Prognosen zu stellen. Beim Maschinellen Lernen ist es wichtig, dass das Modell in der Lernphase nicht zu stark angepasst wird, da es sonst bei neuen Daten zu falschen Ergebnissen kommen kann. Ein guter Ansatz beim Maschinellen Lernen ist es, die vorhandenen Daten in Lern- und Testdaten aufzuteilen. Da bei den Testdaten das Ergebnis schon bekannt ist, kann man die Erfolgsquote des Maschinellen Lernens einfach ueberpruefen.
Klassifizierung und Clustering
Zwei wichtige Aspekte bei Big Data Analysen sind Klassifizierung und Clustering. Klassifizierung wird verwendet, um Daten in verschiedene Gruppen aufzuteilen. Im Gegensatz zu Clustering werden die Daten in Verbindung mit einem Trainingsdatensatz ausgewertet. Die Feature Variablen werden in Gruppen aufgeteilt, die jeweils eine Form der Zielvariable darstellen. Beim Clustering wird lediglich nach aehnlichkeiten der Feature Variablen unterschieden, ohne die Struktur der Gruppen zu verstehen. Klassifizierung wird auch als Beobachtetes Lernen bezeichnet, wobei Clustering als Unbeobachtetes Lernen bezeichnet wird (Aggarwal, 2014).
Um verschiedene Arten von Angriffen zu erkennen ist es bei der Intrusion Detection hilfreich, verschiedene Klassifikatoren (und damit Beobachtetes Lernen) zu verwenden (Kulariya et al., 2016).
Zur Klassifizierung gibt es verschiedene Algorithmen und Verfahren. Nachfolgend werden einige dieser Verfahren genauer beschrieben.
Logistische Regression
Die logistische Regression ist eine Regressionsanalyse, die die Verteilung von voneinander abhaengigen Variablen modelliert. Das Ziel der logistischen Regression ist es, eine direkte Verteilung von Feature Variablen aus einem Trainingsdatensatz darzustellen. Das Ergebnis der logistischen Regression sind Wahrscheinlichkeitsaussagen. Ein Problem der logistischen Regression ist es, dass haeufig ueberanpassung auftritt, wenn zufaellige Fehler im Trainingsdatensatz vorhanden sind oder der Trainingsdatensatz relativ klein ist und aus vielen Dimensionen besteht. Es sollte vermieden werden, das Modell zu stark an den Trainingsdatensatz anzupassen, da es sonst eine schlechte Genauigkeit voraussagt indem es kleinere Abweichungen der Daten zu hoch einschaetzt.
Die logistische Regression wird haeufig in web, medizinischen und sozialwissenschaftlichen Feldern angewandt. In der Wirtschaft wird diese Technik dafuer verwendet, das Scheitern von Prozessen, Systemen oder Produkten vorauszusagen. Ausserdem wird das Verfahren zu Marketingzwecken eingesetzt, um beispielsweise die Kaufwahrscheinlichkeit eines Kunden fuer ein spezielles Produkt abzuwaegen (Deng, Sun, Chang, & Han, 2014).
Naive Bayes
Das Naive Bayes Verfahren wird unter den Wahrscheinlichkeitsmethoden eingeordnet. Es bedient sich vereinfachter Annahmen, daher auch die Bezeichnung naiv, die mit dem Theorem von Bayes verknuepft werden. Dadurch kann fuer jedes Feature eines Testdatensatzes eine Wahrscheinlichkeit erstellt werden, die mit anderen Feature-Wahrscheinlichkeiten multipliziert werden koennen (Aggarwal, 2014). Die Klassifikatoren des Naive Bayes Verfahrens sind skalierbar und koennen effizient trainiert werden. In
Intrusion Detection mit Hilfe von Big Data Analyseverfahren

einem einzigen Durchlauf der Trainingsdaten kann der Algorithmus die bedingten Wahrscheinlichkeiten eines jeden Features berechnen (Kulariya et al., 2016).
Beim Beispiel eines Spamfilters koennten die einzelnen Woerter des Betreffs als Features angesehen werden. Jedes Wort, das dadurch im Testdatensatz gelernt wird, wird mit einer Wahrscheinlichkeit versehen, mit der es in einer Spamnachricht verwendet wird. Die Wahrscheinlichkeiten werden miteinander multipliziert und sobald die Gesamtwahrscheinlichkeit einen Schwellenwert ueberschreitet wird die Mail als Spam gekennzeichnet.
Falls es notwendig ist, kann beim Naive Bayes Verfahren auch eine Laplace Glaettung durchgefuehrt werden. Das ist besonders sinnvoll, wenn der Beispieldatensatz nicht gross genug ist. Durch die Laplace Glaettung werden die Ergebnisse zwar ein wenig ungenauer fuer den Trainingsdatensatz, allerdings sinkt dadurch die Fehlerquote fuer neue Auswertungen, weil das Verfahren sich nicht zu stark an den Trainingsdatensatz anpasst. Trotz der naiven Annahmen und der Vereinfachung der Daten erhaelt man mit dem Naive Bayes Verfahren gute Ergebnisse. Es wird in einer Vielzahl verschiedener Anwendungsfaelle verwendet, vor allem in der Texterkennung (Aggarwal, 2014).
Support Vector Machine
Eine Support Vector Machine (SVM) ist ein Ansatz des Maschinellen Lernens, der auf der Statistischen Lerntheorie basiert. Das Ziel einer SVM ist es, den Aufteilungsspielraum zu maximieren, sowie die Anzahl der Trainingsfehler zu minimieren (Rui et al., 2009). Support Vector Machines erstellen eine Hyperebene oder ein Set aus Hyperebenen in Mehrdimensionalen Raeumen um Klassifizierungen, Regressionen oder aehnliche Aufgaben zu bewaeltigen (Kulariya et al., 2016). Sie sind bestens dafuer geeignet, binaere Klassifikationsprobleme zu loesen (Aggarwal, 2014). Dadurch eignen sie sich sehr gut fuer die Intrusion Detection, da eine Aktivitaet nur als gefaehrlich oder ungefaehrlich eingestuft werden soll. Dementsprechend soll entweder eine Warnung ausgeloest werden, oder nicht.
SVM Optimierungstechniken sind heutzutage ziemlich ausgereift, wodurch sie weitlaeufig in verschiedenen Anwendungsfeldern verwendet werden (Wang & Lin, 2014).
Apache Spark
Apache Spark ist ein Werkzeug, das speziell zur Datenverarbeitung von Big Data entwickelt wurde. Im Vergleich zu anderen Werkzeugen zur Big Data Verarbeitung, beispielsweise Apache Hadoop und Apache Storm, verwendet es mehrstufige in-memory Verarbeitungsprozesse, wodurch es bis zu 100-mal schneller Daten verarbeiten kann als mit map-reduce Verarbeitungsprozessen. Apache Spark unterstuetzt zudem verschiedene Programmiersprachen wie Java oder Python und bietet eine benutzerfreundliche API und Shells fuer Python, Scala, Java und SQL an. Der Kern von Apache Spark besteht aus Basisfunktionalitaeten wie Task Scheduling, Memory Management oder die Interaktion mit Speichersystemen (Kulariya et al., 2016)